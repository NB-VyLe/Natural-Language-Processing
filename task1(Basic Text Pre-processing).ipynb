{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Milestone I Natural Language Processing\n",
    "## Task 1. Basic Text Pre-processing\n",
    "#### Student Name: NGOC BAO VY LE\n",
    "#### Student ID: 3828276\n",
    "\n",
    "Date: 03/10/2021\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "## Introduction\n",
    "In this task, we performs basic text pre-processing on the jobAds dataset, including tokenization, removing most/less frequent words and stop words, extracting bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to import libraries as you need in this assessment, e.g.,\n",
    "from sklearn.datasets import load_files\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from nltk.probability import *\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Examining and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data by load_files() , and return a data in a dictionary form\n",
    "data = load_files(r\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check data type\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all the key values by keys() \n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: 55449\n",
      "filenames: 55449\n",
      "\n",
      "There are 8 target names\n",
      "target_names: ['Accounting_Finance', 'Engineering', 'Healthcare_Nursing', 'Hospitality_Catering', 'IT', 'PR_Advertising_Marketing', 'Sales', 'Teaching']\n",
      "\n",
      "target:  55449\n",
      "{0, 1, 2, 3, 4, 5, 6, 7}\n",
      "\n",
      "DESCR:  None\n"
     ]
    }
   ],
   "source": [
    "# check the data infor\n",
    "print('data:', len(data['data']))\n",
    "print('filenames:', len(data.filenames))\n",
    "print()\n",
    "print('There are', len(data.target_names), 'target names')\n",
    "print('target_names:',  data.target_names)\n",
    "print()\n",
    "print('target: ', len(data.target))\n",
    "print(set(data.target))\n",
    "print()\n",
    "print('DESCR: ', data.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pre-processing data\n",
    "Perform the required text pre-processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract stop words into a list\n",
    "stopwords_en = []\n",
    "with open('./stopwords_en.txt') as f:\n",
    "    stopwords_en = f.read().splitlines()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function to check the stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it takes a tokenize jobAds list and display the number of vocab, words, Lexical diversity, max/min/average review length\n",
    "\n",
    "def stats_print(tk_jobAds):\n",
    "    \n",
    "    # we put all the tokens in the corpus in a single list\n",
    "    words = list(chain.from_iterable(tk_jobAds)) \n",
    "    \n",
    "    # compute the vocabulary by converting the list of words/tokens to a set, i.e., giving a set of unique words\n",
    "    vocab = set(words) \n",
    "    \n",
    "    lexical_diversity = len(vocab)/len(words)\n",
    "    \n",
    "    print(\"Vocabulary size: \",len(vocab))\n",
    "    print(\"Total number of tokens: \", len(words))\n",
    "    print(\"Lexical diversity: \", lexical_diversity)\n",
    "    print(\"Total number of reviews:\", len(tk_jobAds))\n",
    "    \n",
    "    # compute the number of tokens of each job Ad \n",
    "    lens = [len(j) for j in tk_jobAds]\n",
    "    \n",
    "    print(\"Average review length:\", np.mean(lens))\n",
    "    print(\"Maximun review length:\", np.max(lens))\n",
    "    print(\"Minimun review length:\", np.min(lens))\n",
    "    print(\"Standard deviation of review length:\", np.std(lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 pre-processing the frist jobAd for testing purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The the first jobAd description has 173 words after tokenizing\n"
     ]
    }
   ],
   "source": [
    "# extract the 1st jobAd\n",
    "sample = data.data[0]\n",
    "\n",
    "# decode it into utf-8\n",
    "sample = sample.decode('utf-8')\n",
    "\n",
    "# split jobAd by new line\n",
    "sample = re.split(r'\\n', sample)\n",
    "\n",
    "# extract the description and ommit \"Description: \" string\n",
    "sample = [re.sub('^Description:\\s*', '', i) for i in sample if i.startswith('Description:')] \n",
    "\n",
    "# normalize description into lower case\n",
    "sample = sample[0].lower()\n",
    "\n",
    "# tokenize description into sentences\n",
    "sample_sentences = sent_tokenize(sample)\n",
    "\n",
    "# create a pattern to tokenize the whole jobAd\n",
    "pattern = r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"\n",
    "tokenizer = RegexpTokenizer(pattern)\n",
    "\n",
    "# tokenize each sentence in jobAd\n",
    "sample_tokens = [tokenizer.tokenize(sent) for sent in sample_sentences]\n",
    "\n",
    "# flattern all the tokens into 1 list\n",
    "sample_tokens= list(chain.from_iterable(sample_tokens)) \n",
    "\n",
    "print('The the first jobAd description has %d words after tokenizing' %len(sample_tokens))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing the words with less than 2 character: 173\n",
      "After removing: 167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['our',\n",
       " 'client',\n",
       " 'has',\n",
       " 'established',\n",
       " 'itself',\n",
       " 'as',\n",
       " 'leading',\n",
       " 'manufacturer',\n",
       " 'and',\n",
       " 'supplier',\n",
       " 'of',\n",
       " 'quality',\n",
       " 'water',\n",
       " 'treatment',\n",
       " 'plants',\n",
       " 'ranging',\n",
       " 'from',\n",
       " 'basic',\n",
       " 'water',\n",
       " 'softeners',\n",
       " 'and',\n",
       " 'reverse',\n",
       " 'osmosis',\n",
       " 'equipment',\n",
       " 'to',\n",
       " 'customer',\n",
       " 'specified',\n",
       " 'complex',\n",
       " 'water',\n",
       " 'treatment',\n",
       " 'solutions',\n",
       " 'the',\n",
       " 'company',\n",
       " 'are',\n",
       " 'able',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'their',\n",
       " 'clients',\n",
       " 'requirements',\n",
       " 'through',\n",
       " 'flexibility',\n",
       " 'in',\n",
       " 'tailoring',\n",
       " 'their',\n",
       " 'product',\n",
       " 'to',\n",
       " 'their',\n",
       " 'needs',\n",
       " 'and',\n",
       " 'budgets',\n",
       " 'due',\n",
       " 'to',\n",
       " 'expansion',\n",
       " 'and',\n",
       " 'an',\n",
       " 'increased',\n",
       " 'workload',\n",
       " 'they',\n",
       " 'are',\n",
       " 'seeking',\n",
       " 'to',\n",
       " 'recruit',\n",
       " 'planet',\n",
       " 'engineer',\n",
       " 'to',\n",
       " 'cover',\n",
       " 'accounts',\n",
       " 'along',\n",
       " 'the',\n",
       " 'corridor',\n",
       " 'responsibilities',\n",
       " 'will',\n",
       " 'include',\n",
       " 'conducting',\n",
       " 'the',\n",
       " 'routine',\n",
       " 'sampling',\n",
       " 'and',\n",
       " 'analysis',\n",
       " 'of',\n",
       " 'water',\n",
       " 'systems',\n",
       " 'interpreting',\n",
       " 'results',\n",
       " 'maintenance',\n",
       " 'and',\n",
       " 'the',\n",
       " 'installation',\n",
       " 'of',\n",
       " 'chemical',\n",
       " 'dosing',\n",
       " 'systems',\n",
       " 'servicing',\n",
       " 'accounts',\n",
       " 'within',\n",
       " 'both',\n",
       " 'the',\n",
       " 'industrial',\n",
       " 'and',\n",
       " 'commercial',\n",
       " 'industries',\n",
       " 'the',\n",
       " 'successful',\n",
       " 'candidate',\n",
       " 'will',\n",
       " 'complete',\n",
       " 'all',\n",
       " 'work',\n",
       " 'in',\n",
       " 'accordance',\n",
       " 'to',\n",
       " 'the',\n",
       " 'approved',\n",
       " 'code',\n",
       " 'of',\n",
       " 'practice',\n",
       " 'the',\n",
       " 'ideal',\n",
       " 'applicant',\n",
       " 'for',\n",
       " 'this',\n",
       " 'position',\n",
       " 'will',\n",
       " 'have',\n",
       " 'minimum',\n",
       " 'of',\n",
       " 'two',\n",
       " 'years',\n",
       " 'relevant',\n",
       " 'industry',\n",
       " 'experience',\n",
       " 'and',\n",
       " 'will',\n",
       " 'have',\n",
       " 'knowledge',\n",
       " 'of',\n",
       " 'reverse',\n",
       " 'osmosis',\n",
       " 'water',\n",
       " 'softeners',\n",
       " 'water',\n",
       " 'filters',\n",
       " 'and',\n",
       " 'uv',\n",
       " 'equipment',\n",
       " 'full',\n",
       " 'uk',\n",
       " 'driving',\n",
       " 'license',\n",
       " 'is',\n",
       " 'essential',\n",
       " 'in',\n",
       " 'return',\n",
       " 'the',\n",
       " 'client',\n",
       " 'is',\n",
       " 'offering',\n",
       " 'competitive',\n",
       " 'benefits',\n",
       " 'and',\n",
       " 'salary',\n",
       " 'package',\n",
       " 'to',\n",
       " 'the',\n",
       " 'ideal',\n",
       " 'candidate']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Before removing the words with less than 2 character: %d' %len(sample_tokens))\n",
    "\n",
    "\n",
    "# remove words with 2 characters\n",
    "sample_tokens = [i for i in sample_tokens if len(i) >1]\n",
    "\n",
    "print(\"After removing: %d\" %len(sample_tokens))\n",
    "sample_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: 167\n",
      "after: 99\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['client',\n",
       " 'established',\n",
       " 'leading',\n",
       " 'manufacturer',\n",
       " 'supplier',\n",
       " 'quality',\n",
       " 'water',\n",
       " 'treatment',\n",
       " 'plants',\n",
       " 'ranging',\n",
       " 'basic',\n",
       " 'water',\n",
       " 'softeners',\n",
       " 'reverse',\n",
       " 'osmosis',\n",
       " 'equipment',\n",
       " 'customer',\n",
       " 'complex',\n",
       " 'water',\n",
       " 'treatment',\n",
       " 'solutions',\n",
       " 'company',\n",
       " 'meet',\n",
       " 'clients',\n",
       " 'requirements',\n",
       " 'flexibility',\n",
       " 'tailoring',\n",
       " 'product',\n",
       " 'budgets',\n",
       " 'due',\n",
       " 'expansion',\n",
       " 'increased',\n",
       " 'workload',\n",
       " 'seeking',\n",
       " 'recruit',\n",
       " 'planet',\n",
       " 'engineer',\n",
       " 'cover',\n",
       " 'accounts',\n",
       " 'corridor',\n",
       " 'responsibilities',\n",
       " 'include',\n",
       " 'conducting',\n",
       " 'routine',\n",
       " 'sampling',\n",
       " 'analysis',\n",
       " 'water',\n",
       " 'systems',\n",
       " 'interpreting',\n",
       " 'results',\n",
       " 'maintenance',\n",
       " 'installation',\n",
       " 'chemical',\n",
       " 'dosing',\n",
       " 'systems',\n",
       " 'servicing',\n",
       " 'accounts',\n",
       " 'industrial',\n",
       " 'commercial',\n",
       " 'industries',\n",
       " 'successful',\n",
       " 'candidate',\n",
       " 'complete',\n",
       " 'work',\n",
       " 'accordance',\n",
       " 'approved',\n",
       " 'code',\n",
       " 'practice',\n",
       " 'ideal',\n",
       " 'applicant',\n",
       " 'position',\n",
       " 'minimum',\n",
       " 'years',\n",
       " 'relevant',\n",
       " 'industry',\n",
       " 'experience',\n",
       " 'knowledge',\n",
       " 'reverse',\n",
       " 'osmosis',\n",
       " 'water',\n",
       " 'softeners',\n",
       " 'water',\n",
       " 'filters',\n",
       " 'uv',\n",
       " 'equipment',\n",
       " 'full',\n",
       " 'uk',\n",
       " 'driving',\n",
       " 'license',\n",
       " 'essential',\n",
       " 'return',\n",
       " 'client',\n",
       " 'offering',\n",
       " 'competitive',\n",
       " 'benefits',\n",
       " 'salary',\n",
       " 'package',\n",
       " 'ideal',\n",
       " 'candidate']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing stop words\n",
    "print('before: %d' %len(sample_tokens))\n",
    "sample_tokens= [ i for i in sample_tokens if i not in stopwords_en]\n",
    "print('after: %d' %len(sample_tokens))\n",
    "sample_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing on the one sample, we can start to tokenize for all jobAd descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Pre-processing all the job ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all jobads and their corresponding lables\n",
    "jobAds , lables = data.data, data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 :Extract  job Description and tokenize each job Ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code does:<br>\n",
    ">read the txt from jobAd<br>\n",
    ">decode the whole job Ad<br>\n",
    ">split the string by new line<br>\n",
    ">extract the lines that start with Description and add them to the list<br>\n",
    ">normalize the description txt to lower charater <br>\n",
    ">subtract the frist description word of the line<br>\n",
    ">tokenize each txt by sentence then by word<br>\n",
    ">flattern the list of list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_jobAds(raw_string):\n",
    "    \n",
    "   \n",
    "    raw_string = raw_string.decode('utf-8')# decode jobad\n",
    "    raw_string = re.split(r'\\n', raw_string) # split jobad by new line\n",
    "         \n",
    "    \n",
    "    # extract the description only\n",
    "    elements = [i for i in raw_string if i.startswith('Description:')] \n",
    "    \n",
    "    \n",
    "    description = re.sub(r'^Description:\\s*', '', elements[0]).lower()# normalize description to lower case\n",
    "    \n",
    "    sentences = sent_tokenize(description ) # tokenize by sentence\n",
    "    \n",
    "    pattern = r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\" # create a tokenizer pattern\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    \n",
    "    # tokernize each sentence in description into tokens\n",
    "    tokenize_each_sentence = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "    \n",
    "    # flattern the all tokens in the description into 1 list\n",
    "    tokens_jobAd = list(chain.from_iterable(tokenize_each_sentence))\n",
    "    \n",
    "        \n",
    "    return tokens_jobAd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize each job Ad\n",
    "tk_jobAds = [tokenize_jobAds(i) for i in jobAds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  89591\n",
      "Total number of tokens:  13799127\n",
      "Lexical diversity:  0.006492512171240978\n",
      "Total number of reviews: 55449\n",
      "Average review length: 248.861602553698\n",
      "Maximun review length: 2001\n",
      "Minimun review length: 10\n",
      "Standard deviation of review length: 125.26507304982165\n"
     ]
    }
   ],
   "source": [
    "# the orginal tokens stat\n",
    "stats_print(tk_jobAds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: remove words with 1 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  89565\n",
      "Total number of tokens:  13342925\n",
      "Lexical diversity:  0.006712546162104636\n",
      "Total number of reviews: 55449\n",
      "Average review length: 240.63418636945661\n",
      "Maximun review length: 1919\n",
      "Minimun review length: 10\n",
      "Standard deviation of review length: 121.91270721028921\n"
     ]
    }
   ],
   "source": [
    "# # for tokens in each job ad, extract the tokens that have more than 1 character\n",
    "tk_jobAds = [[ word for word in t if len(word) >1] for t in tk_jobAds]\n",
    "\n",
    "# Check stat\n",
    "stats_print(tk_jobAds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  89052\n",
      "Total number of tokens:  7863307\n",
      "Lexical diversity:  0.011325006132915833\n",
      "Total number of reviews: 55449\n",
      "Average review length: 141.8115204963119\n",
      "Maximun review length: 1132\n",
      "Minimun review length: 7\n",
      "Standard deviation of review length: 73.78995293014496\n"
     ]
    }
   ],
   "source": [
    "# for tokens in each job ad, extract the tokens that not in the stop word list\n",
    "tk_jobAds = [ [word for word in t if word not in stopwords_en]  for t in tk_jobAds]\n",
    "\n",
    "# Check stat\n",
    "stats_print(tk_jobAds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Remove the word that appears only once in the document collection, based on term frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  40088\n",
      "Total number of tokens:  7814343\n",
      "Lexical diversity:  0.005130053799788415\n",
      "Total number of reviews: 55449\n",
      "Average review length: 140.9284748146946\n",
      "Maximun review length: 1121\n",
      "Minimun review length: 7\n",
      "Standard deviation of review length: 73.46663506985078\n"
     ]
    }
   ],
   "source": [
    "# get all words from tk_jobAds by flatten the tk_jobAds list\n",
    "words = list(chain.from_iterable(tk_jobAds))\n",
    "\n",
    "# get the list of words with it's frequence\n",
    "term_fd = FreqDist(words)\n",
    "\n",
    "# extract the words that appearces only once in the\n",
    "lessFreqWords = set(term_fd.hapaxes())\n",
    "\n",
    "# remove word in lessFreqWords\n",
    "tk_jobAds = [[ word for word in t if word not in lessFreqWords] for t in tk_jobAds]\n",
    "\n",
    "# check stats\n",
    "stats_print(tk_jobAds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Remove the top 50 most frequent words based on document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ability',\n",
       " 'apply',\n",
       " 'based',\n",
       " 'benefits',\n",
       " 'business',\n",
       " 'candidate',\n",
       " 'candidates',\n",
       " 'client',\n",
       " 'company',\n",
       " 'contact',\n",
       " 'cv',\n",
       " 'development',\n",
       " 'environment',\n",
       " 'essential',\n",
       " 'excellent',\n",
       " 'experience',\n",
       " 'experienced',\n",
       " 'good',\n",
       " 'high',\n",
       " 'including',\n",
       " 'job',\n",
       " 'jobseeking',\n",
       " 'join',\n",
       " 'key',\n",
       " 'knowledge',\n",
       " 'leading',\n",
       " 'level',\n",
       " 'management',\n",
       " 'manager',\n",
       " 'opportunity',\n",
       " 'originally',\n",
       " 'position',\n",
       " 'posted',\n",
       " 'provide',\n",
       " 'recruitment',\n",
       " 'required',\n",
       " 'role',\n",
       " 'salary',\n",
       " 'service',\n",
       " 'services',\n",
       " 'skills',\n",
       " 'strong',\n",
       " 'successful',\n",
       " 'support',\n",
       " 'team',\n",
       " 'training',\n",
       " 'uk',\n",
       " 'work',\n",
       " 'working',\n",
       " 'www'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the set tokens for each job ad\n",
    "set_words = list(chain.from_iterable([set(j) for j in tk_jobAds]))\n",
    "\n",
    "# generate the frequence of for each word\n",
    "doc_fd = FreqDist(set_words)\n",
    "\n",
    "\n",
    "doc_fd.most_common(50)\n",
    "mostCommon =set([ i[0] for i in doc_fd.most_common(50)])\n",
    "mostCommon\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  40038\n",
      "Total number of tokens:  6239169\n",
      "Lexical diversity:  0.0064172007522155594\n",
      "Total number of reviews: 55449\n",
      "Average review length: 112.52085700373316\n",
      "Maximun review length: 990\n",
      "Minimun review length: 4\n",
      "Standard deviation of review length: 61.88637513583753\n"
     ]
    }
   ],
   "source": [
    "# remove 50 most frequent words\n",
    "tk_jobAds = [[word for word in t if word not in mostCommon] for t in tk_jobAds]\n",
    "\n",
    "#Check stat\n",
    "stats_print(tk_jobAds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Extract the top 10 Bigrams based on term frequency, save them as a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('employment', 'agency'), 8055),\n",
       " (('track', 'record'), 5472),\n",
       " (('acting', 'employment'), 5095),\n",
       " (('sql', 'server'), 4804),\n",
       " (('asp', 'net'), 4687),\n",
       " (('relation', 'vacancy'), 3977),\n",
       " (('sales', 'executive'), 3619),\n",
       " (('chef', 'de'), 3586),\n",
       " (('nursing', 'home'), 3503),\n",
       " (('de', 'partie'), 3396)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatten tk_jobAds list\n",
    "words = list(chain.from_iterable(tk_jobAds))\n",
    "\n",
    "# Extracting from a text a list of bi-gram \n",
    "bigrams = ngrams(words, n = 2)\n",
    "\n",
    "# generate the bigrams frequence\n",
    "fdbigram = FreqDist(bigrams)\n",
    "\n",
    "# Extract top 10 Bigrams\n",
    "most_common_bigrams = fdbigram.most_common(10)\n",
    "\n",
    "most_common_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# convert top 10 Bigrams into list with format [frequency, words]\n",
    "#  [8055, 'employment agency']\n",
    "bigrams_pattern = [[element[1], \" \".join(element[0])] for element in most_common_bigrams]\n",
    "\n",
    "# sort  bigrams_pattern by the frequency\n",
    "sorted(bigrams_pattern, reverse = True)\n",
    "\n",
    "# put back to wrods,frequency form and type cast frequency into str\n",
    "#['employment agency', 8055]\n",
    "bigrams = [[ element[1], str(element[0])] for element in bigrams_pattern]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3  Saving required outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Saving job_ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find web Index for each job ad\n",
    "\n",
    "def extract_webIndexes(raw_string):\n",
    "    \n",
    "    '''\n",
    "        function take a str, decode and split it by new line\n",
    "        find the elements start with webIndexes\n",
    "        Extract the digit after webIndexes:\n",
    "        Add them to a list\n",
    "    '''\n",
    "    \n",
    "    # decode jobad\n",
    "    raw_string = raw_string.decode('utf-8')\n",
    "    \n",
    "    # split jobad into a list of lines\n",
    "    raw_string = re.split(r'\\n', raw_string) \n",
    "    \n",
    "    # pick the line with Webindex then extract the digit number\n",
    "    webIndexes = [re.sub(r'^Webindex:\\s*', '', i) for i in raw_string if i.startswith('Webindex:')]\n",
    "    \n",
    "    return webIndexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flattern the list of webIndexes\n",
    "webIndexes  = list(chain.from_iterable([extract_webIndexes(i) for i in jobAds ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the corresponding name for each target lable\n",
    "\n",
    "# dict of categories\n",
    "target_names_dict = {'Accounting_Finance':0, 'Engineering':1, 'Healthcare_Nursing':2, 'Hospitality_Catering':3,\n",
    "                    'IT':4, 'PR_Advertising_Marketing':5, 'Sales':6, 'Teaching':7}\n",
    "\n",
    "# make a copy of target labels and convert it into list\n",
    "categories = lables.tolist()\n",
    "\n",
    "# converting lable to the corresponding categories \n",
    "for i in range(len(categories)):\n",
    "    for key, value in target_names_dict.items():\n",
    "        if categories[i] == value:\n",
    "            categories[i] = key  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding title for each job ad\n",
    "\n",
    "def extract_Titles(raw_string):\n",
    "    \n",
    "    '''\n",
    "        function take a str, decode and split it by new line\n",
    "        find the elements start with Title \n",
    "        Extract the txt after Title:\n",
    "        Add them to a list\n",
    "    '''\n",
    "    \n",
    "    # decode jobad\n",
    "    raw_string = raw_string.decode('utf-8')\n",
    "    \n",
    "    # split jobad by new line\n",
    "    raw_string = re.split(r'\\n', raw_string) \n",
    "    \n",
    "    # pick the lines start with Title, extract the string after title\n",
    "    titles = [re.sub(r'^Title:\\s*', '', i) for i in raw_string if i.startswith('Title:')]\n",
    "    \n",
    "    return titles   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flattern the list of titles \n",
    "titles =  list(chain.from_iterable([extract_Titles(i) for i in jobAds ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert filenames to list\n",
    "fileNames = data.filenames.tolist()\n",
    "\n",
    "# extract the 5 digit from file name by subtite non digit pattern to empty string\n",
    "ids = [re.sub(r'[^\\d{5}]', '', i) for i in fileNames]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the job_ads.txt to save Id, title, webindex and job description\n",
    "\n",
    " # create a new txt file to write\n",
    "jobads = open('job_ads.txt ', 'w')\n",
    "\n",
    "# for each job ad, we save them with the format\n",
    "'''\n",
    "ID:<5digit id>\n",
    "Category: <category>\n",
    "Webindex: <8 digit web index>\n",
    "Title:<title of the advertisement>\n",
    "Description: <description of the advertisement>\n",
    "'''\n",
    "string = '\\n'.join(['\\n'.join(('ID: ' + ids[i],'Category: ' + categories[i], 'Webindex: ' + webIndexes[i] , \n",
    "            'Title: ' + titles[i], \"Description: \" + ' '.join(tk_jobAds[i]))) for i in range(len(jobAds))])\n",
    "\n",
    "# save each review in file\n",
    "jobads.write(string) \n",
    "\n",
    "# close the file\n",
    "jobads.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Saving vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the ordered vocal by alphabet order\n",
    "vocal  = sorted(list(set(words)))\n",
    "\n",
    "# create a new txt file to write\n",
    "vocabulary = open('vocab.txt', 'w') \n",
    "\n",
    "# loop through vocal and extract each vocal and its index\n",
    "# format : each line is word:index\n",
    "string = '\\n'.join( ':'.join([vocal[i], str(i)]) for i in range(len(vocal)))\n",
    "\n",
    "# save each review in file       \n",
    "vocabulary.write(string)\n",
    "\n",
    "# close the file\n",
    "vocabulary.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Saving bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    " # create a new txt file to write\n",
    "out_file = open('bigram.txt', 'w')\n",
    "    \n",
    "# loop through list of bigrams to extract it values\n",
    "string = '\\n'.join( ','.join(ele) for ele in bigrams)\n",
    "    \n",
    " # save each review in file     \n",
    "out_file.write(string)\n",
    "\n",
    "#  close the file\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this task, we have process job description by remove:<br>\n",
    "-stop word<br>\n",
    "-word with 1 character<br> \n",
    "-word that appeared once by term<br>\n",
    "-the top 50 most frequent words based on document frequency<br>\n",
    "We have reduce the vocabulary size by over 44%\n",
    "<br>\n",
    "The processed job descriptions, vocab and bigram words are save in text files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
